{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGC5zBa03SRC"
      },
      "source": [
        "# Using GloVe Embedding\n",
        "\n",
        "© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "**Author:** Axel Sirota\n",
        "\n",
        "\n",
        "In this notebook we will leverage Standford's GloVe vectors which is a pretrained embedding on 1.4B Tweets.\n",
        "\n",
        "Take it easy and pay attention to the main differences with before, and the non-trainable parameters. Finally, check how accurate the results now are.\n",
        "You can run this lab both locally or in Colab.\n",
        "\n",
        "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
        "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
        "\n",
        "Follow the instructions. Good luck!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75x9mZNIbPAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6eb2b0d-5882-4459-896e-beea989461bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 13 08:24:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUsYzWxfT8o-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f021d85d-c606-4ccb-fcd2-5f603f4c61f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting keras-nlp\n",
            "  Downloading keras_nlp-0.11.1-py3-none-any.whl (515 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.2.0\n",
            "  Downloading gensim-4.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text==2.15.0\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.2.0) (6.4.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Collecting keras-core (from keras-nlp)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (24.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (2023.12.25)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text==2.15.0) (2.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-nlp) (2.31.0)\n",
            "Collecting namex (from keras-core->keras-nlp)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (2.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2024.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.2.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56441 sha256=ac00e9ccd36fb809d5183f7c23d324f542cd1b98484e2d6ffe87e52a55d5a75b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: namex, np_utils, keras-preprocessing, gensim, keras-core, tensorflow-text, keras-nlp\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.2\n",
            "    Uninstalling gensim-4.3.2:\n",
            "      Successfully uninstalled gensim-4.3.2\n",
            "Successfully installed gensim-4.2.0 keras-core-0.1.7 keras-nlp-0.11.1 keras-preprocessing-1.1.2 namex-0.0.8 np_utils-0.6.0 tensorflow-text-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'gensim==4.2.0' 'tensorflow-text==2.15.0' np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3XUwgb0UBut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162d1f56-5fb8-49d5-98a7-26a815fa6e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "TRACE = False\n",
        "embedding_dim = 100\n",
        "epochs=2\n",
        "batch_size = 64\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx3hZd6gUImG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa1d1d8-daae-432c-9568-fb73de13387d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing get_data.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f yelp.csv ]; then\n",
        "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
        "fi\n",
        "\n",
        "if [ ! -f glove.6B.100d.txt ]; then\n",
        "  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfyMUL8nXRYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9380b3db-4c37-4e79-c45f-d90897705136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-13 08:28:15--  https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/xds4lua69b7okw8/yelp.csv [following]\n",
            "--2024-05-13 08:28:15--  https://www.dropbox.com/s/raw/xds4lua69b7okw8/yelp.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3fe7c7a80f39b065cb7f8b0de5.dl.dropboxusercontent.com/cd/0/inline/CSyoJSvn_MilY09gV8UlukeEdxwAR5mTpnlZ2I0Wm6dig7Z5RxqWjsdROKQgvdterDSrJl-tOl-OKtLVfDgHOuH91jLwwv9ajevJUXoyO8bcM6TxLZCf5riAgRE6Lc-4a-okpD9W8v6lrVoMNZCGaOxO/file# [following]\n",
            "--2024-05-13 08:28:16--  https://uc3fe7c7a80f39b065cb7f8b0de5.dl.dropboxusercontent.com/cd/0/inline/CSyoJSvn_MilY09gV8UlukeEdxwAR5mTpnlZ2I0Wm6dig7Z5RxqWjsdROKQgvdterDSrJl-tOl-OKtLVfDgHOuH91jLwwv9ajevJUXoyO8bcM6TxLZCf5riAgRE6Lc-4a-okpD9W8v6lrVoMNZCGaOxO/file\n",
            "Resolving uc3fe7c7a80f39b065cb7f8b0de5.dl.dropboxusercontent.com (uc3fe7c7a80f39b065cb7f8b0de5.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc3fe7c7a80f39b065cb7f8b0de5.dl.dropboxusercontent.com (uc3fe7c7a80f39b065cb7f8b0de5.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8091185 (7.7M) [text/plain]\n",
            "Saving to: ‘yelp.csv’\n",
            "\n",
            "yelp.csv            100%[===================>]   7.72M  20.4MB/s    in 0.4s    \n",
            "\n",
            "2024-05-13 08:28:16 (20.4 MB/s) - ‘yelp.csv’ saved [8091185/8091185]\n",
            "\n",
            "--2024-05-13 08:28:16--  https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/dl1vswq2sz5f1ws/glove.6B.100d.txt [following]\n",
            "--2024-05-13 08:28:17--  https://www.dropbox.com/s/raw/dl1vswq2sz5f1ws/glove.6B.100d.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6e0d62943f806ee6533568bc44.dl.dropboxusercontent.com/cd/0/inline/CSxLps41hkprv-MxrvcOGbCkcuVpZwANe08pGcEqydmhjUiI3GTXfGdZv9T4PrHuSsjXzERpYWPNfbiLaQ_AwV6nukoLJ97AfM0c7w0hcOa4WBWxfYkuMWIl8oBF-1aA2lG-TeDdPIhJzIZaHBgA1UVy/file# [following]\n",
            "--2024-05-13 08:28:17--  https://uc6e0d62943f806ee6533568bc44.dl.dropboxusercontent.com/cd/0/inline/CSxLps41hkprv-MxrvcOGbCkcuVpZwANe08pGcEqydmhjUiI3GTXfGdZv9T4PrHuSsjXzERpYWPNfbiLaQ_AwV6nukoLJ97AfM0c7w0hcOa4WBWxfYkuMWIl8oBF-1aA2lG-TeDdPIhJzIZaHBgA1UVy/file\n",
            "Resolving uc6e0d62943f806ee6533568bc44.dl.dropboxusercontent.com (uc6e0d62943f806ee6533568bc44.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc6e0d62943f806ee6533568bc44.dl.dropboxusercontent.com (uc6e0d62943f806ee6533568bc44.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347117594 (331M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.txt’\n",
            "\n",
            "glove.6B.100d.txt   100%[===================>] 331.04M  16.2MB/s    in 21s     \n",
            "\n",
            "2024-05-13 08:28:39 (15.7 MB/s) - ‘glove.6B.100d.txt’ saved [347117594/347117594]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ckCWLwTXVa0"
      },
      "outputs": [],
      "source": [
        "path = './yelp.csv'\n",
        "yelp = pd.read_csv(path)\n",
        "# Create a new DataFrame that only contains the 5-star and 1-star reviews to have extremes.\n",
        "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
        "X = yelp_best_worst.text\n",
        "y = yelp_best_worst.stars.map({1:0, 5:1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV8JS-8harja"
      },
      "outputs": [],
      "source": [
        "corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B__Of4dUXXBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4056276b-6bb3-4bfc-f876-0dfda740b4d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "# Construct a function that fills the embedding_index dict for every word in the GloVe file with its coefficients.\n",
        "# HELP: For that iterate over the Glove file (hint: check that file to view its structure first!), split the word from the numbers, and populate the dictionary with the word and the numbers as a numpy array.\n",
        "# Hint2: check np.fromstring\n",
        "# FILL\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_glove_file) as f:\n",
        "  lines = f.readlines()"
      ],
      "metadata": {
        "id": "4v9IzOBoFk4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in lines:\n",
        "  data = line.split()\n",
        "  embeddings_index[data[0]] = np.array(data[1:])\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gnh4CIgGwx8",
        "outputId": "eb6e78df-cc41-42f8-c6e5-24d0989eb3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fygJjt56Xhup"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n",
        "nb_samples = sum(len(s) for s in corpus)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfkDahA5XnzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433edbf4-8668-4675-bbab-fb9ee7739788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 corpus items are ['My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!', 'I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\n\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we\\'ll be seated when the girl comes back from seating someone else. We were seated at 5:52 and the waiter came and got our drink orders. Everyone was very pleasant from the host that seated us to the waiter to the server. The prices were very good as well. We placed our orders once we decided what we wanted at 6:02. We shared the baked spaghetti calzone and the small \"Here\\'s The Beef\" pizza so we can both try them. The calzone was huge and we got the smallest one (personal) and got the small 11\" pizza. Both were awesome! My friend liked the pizza better and I liked the calzone better. The calzone does have a sweetish sauce but that\\'s how I like my sauce!\\n\\nWe had to box part of the pizza to take it home and we were out the door by 6:42. So, everything was great and not like these bad reviewers. That goes to show you that  you have to try these things yourself because all these bad reviewers have some serious issues.', \"Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\n\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\n\\nThe fenced in area is huge to let the dogs run, play, and sniff!\", 'General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I\\'d be surprised if you don\\'t walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it\\'s how we recover from them that is important\"!!!\\n\\nThanks to Scott and his awesome staff. You\\'ve got a customer for life!! .......... :^)', 'Drop what you\\'re doing and drive here. After I ate here I had to go back the next day for more.  The food is that good.\\n\\nThis cute little green building may have gone competely unoticed if I hadn\\'t been driving down Palm Rd to avoid construction.  While waiting to turn onto 16th Street the \"Grand Opening\" sign caught my eye and my little yelping soul leaped for joy!  A new place to try!\\n\\nIt looked desolate from the outside but when I opened the door I was put at easy by the decor, smell and cleanliness inside.  I ordered dinner for two, to go.  The menu was awesome.  I loved seeing all the variety: poblano peppers, mole, mahi mahi, mushrooms...something wrapped in banana leaves.  It made it difficult to choose something.  Here\\'s what I\\'ve had so far: La Condesa Shrimp Burro and Baja Sur Dogfish Shark Taco.  They are both were very delicious meals but the shrimp burro stole the show.  So much flavor.  I snagged some bites from my hubbys mole and mahi mahi burros- mmmm such a delight.  The salsa bar is endless.  I really stocked up.  I was excited to try the strawberry salsa but it was too hot, in fact it all was, but I\\'m a big wimp when it comes to hot peppers. The horchata is handmade and delicious.  They throw pecans and some fruit in there too which is a yummy bonus!\\n\\nAs if the good food wasn\\'t enough to win me over the art in this restaurant sho did!  I\\'m a sucker for Mexican folk art and Frida Kahlo is my Oprah.  There\\'s a painting of her and Diego hanging over the salsa bar, it\\'s amazing.  All the paintings are great, love the artist.']\n",
            "Length of corpus is 4056\n"
          ]
        }
      ],
      "source": [
        "print(f'First 5 corpus items are {corpus[:5]}')\n",
        "print(f'Length of corpus is {len(corpus)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp6wXMtvXoJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82627a85-62cc-49a5-919d-63aedb9f7402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17190\n",
            "2759\n"
          ]
        }
      ],
      "source": [
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Create a loop such that for every word in the vocabulary, if it exists in the Glove embedding, then set for that word (that means that index) the tensor of the Glove Embedding. Otherwise fill with 0\n",
        "# FILL\n",
        "\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "  glv_embedding = embeddings_index.get(word, None)\n",
        "  if glv_embedding is not None:\n",
        "    embedding_matrix[idx] = glv_embedding\n",
        "    hits += 1\n",
        "  else:\n",
        "    misses += 1\n",
        "\n",
        "print(hits)\n",
        "print(misses)\n",
        "\n",
        "# In the end, the embedding matrix should have, for every word of our tokenizer that exists in GloVe, the tensor representation of that word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "T1nRW7STX3sr"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# Add the new embedding and set it as non-trainable (although you could fine-tune it if you prefer)\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\n",
        "# Add a Lambda layer to average out the words dimension\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
        "# Add a Dense layer to classify words\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CmuPaNl4YEc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bb14d9-187a-4d8b-be77-34a23abae0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 100)         1995000   \n",
            "                                                                 \n",
            " lambda_2 (Lambda)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 19950)             2014950   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4009950 (15.30 MB)\n",
            "Trainable params: 2014950 (7.69 MB)\n",
            "Non-trainable params: 1995000 (7.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Ut0I9Q3SRM"
      },
      "source": [
        "Notice the Non-trainable parameters! What we are doing is just training the softmax based on correct embeddings. This is called fine tuning the embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_433LAsvYHW-"
      },
      "outputs": [],
      "source": [
        "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
        "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=10,  batch_size=250):\n",
        "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
        "    for batch in range(number_of_sentence_batches):\n",
        "        lower_end = batch*batch_size\n",
        "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
        "        mini_batch_size = upper_end - lower_end\n",
        "        maxlen = window_size*2\n",
        "        X = []\n",
        "        Y = []\n",
        "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
        "            L = len(words)\n",
        "            for index, word in enumerate(words):\n",
        "                contexts = []\n",
        "                labels   = []\n",
        "                s = index - window_size\n",
        "                e = index + window_size + 1\n",
        "\n",
        "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
        "                labels.append(word)\n",
        "\n",
        "                x = pad_sequences(contexts, maxlen=maxlen)\n",
        "                y = to_categorical(labels, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "        X = tf.constant(X)\n",
        "        Y = tf.constant(Y)\n",
        "        number_of_batches = len(X) // batch_size\n",
        "        for real_batch in range(number_of_batches):\n",
        "          lower_end = real_batch*batch_size\n",
        "          upper_end = (real_batch+1)*batch_size\n",
        "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
        "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
        "          yield (batch_X, batch_Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterable = generate_data(corpus=tokenized_corpus[:10], vocab_size=vocab_size, batch_size=64)\n",
        "sample_x, sample_y = next(iterable)\n",
        "sample_y_numpy = sample_y.numpy()\n",
        "sample_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWeDKjFWNTz1",
        "outputId": "2122eab4-40bf-4bea-c13d-5b94da690ec4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 4), dtype=int32, numpy=\n",
              "array([[   0,    0,  447,  202],\n",
              "       [   0,   12,  202,   35],\n",
              "       [  12,  447,   35,   41],\n",
              "       [ 447,  202,   41,   20],\n",
              "       [ 202,   35,   20,   12],\n",
              "       [  35,   41,   12,  571],\n",
              "       [  41,   20,  571,   11],\n",
              "       [  20,   12,   11,  282],\n",
              "       [  12,  571,  282,    2],\n",
              "       [ 571,   11,    2,    9],\n",
              "       [  11,  282,    9,    8],\n",
              "       [ 282,    2,    8,  196],\n",
              "       [   2,    9,  196,    1],\n",
              "       [   9,    8,    1, 1549],\n",
              "       [   8,  196, 1549,    8],\n",
              "       [ 196,    1,    8,  201],\n",
              "       [   1, 1549,  201,   71],\n",
              "       [1549,    8,   71,  123],\n",
              "       [   8,  201,  123,  654],\n",
              "       [ 201,   71,  654,  319],\n",
              "       [  71,  123,  319, 4500],\n",
              "       [ 123,  654, 4500,   43],\n",
              "       [ 654,  319,   43, 2394],\n",
              "       [ 319, 4500, 2394,   58],\n",
              "       [4500,   43,   58, 1408],\n",
              "       [  43, 2394, 1408, 1478],\n",
              "       [2394,   58, 1478,   50],\n",
              "       [  58, 1408,   50,  483],\n",
              "       [1408, 1478,  483,    8],\n",
              "       [1478,   50,    8,  196],\n",
              "       [  50,  483,  196,    2],\n",
              "       [ 483,    8,    2,   50],\n",
              "       [   8,  196,   50,   28],\n",
              "       [ 196,    2,   28,  572],\n",
              "       [   2,   50,  572,  664],\n",
              "       [  50,   28,  664,   20],\n",
              "       [  28,  572,   20,    1],\n",
              "       [ 572,  664,    1, 3444],\n",
              "       [ 664,   20, 3444,  458],\n",
              "       [  20,    1,  458,  616],\n",
              "       [   1, 3444,  616,  450],\n",
              "       [3444,  458,  450,    9],\n",
              "       [ 458,  616,    9,  388],\n",
              "       [ 616,  450,  388,   38],\n",
              "       [ 450,    9,   38,    1],\n",
              "       [   9,  388,    1,   27],\n",
              "       [ 388,   38,   27, 4501],\n",
              "       [  38,    1, 4501,   53],\n",
              "       [   1,   27,   53,  178],\n",
              "       [  27, 4501,  178,  664],\n",
              "       [4501,   53,  664,   25],\n",
              "       [  53,  178,   25,    1],\n",
              "       [ 178,  664,    1, 1631],\n",
              "       [ 664,   25, 1631,   15],\n",
              "       [  25,    1,   15,   46],\n",
              "       [   1, 1631,   46,   41],\n",
              "       [1631,   15,   41,    1],\n",
              "       [  15,   46,    1,  138],\n",
              "       [  46,   41,  138,   85],\n",
              "       [  41,    1,   85,  600],\n",
              "       [   1,  138,  600,    4],\n",
              "       [ 138,   85,    4, 1632],\n",
              "       [  85,  600, 1632,    2],\n",
              "       [ 600,    4,    2,   46]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "t8qf2WhiYwOJ"
      },
      "outputs": [],
      "source": [
        "def fit_model():\n",
        "    if not BATCH:\n",
        "        # If we are not batching, Fill how to get X AND Y\n",
        "        X, Y = None # Fill\n",
        "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
        "        model.fit(X, Y, epochs = epochs)\n",
        "    else:\n",
        "        for epoch in range(epochs):\n",
        "          batch_count = 0\n",
        "          for x, y in generate_data(corpus=tokenized_corpus[:100], vocab_size=vocab_size, batch_size=batch_size):\n",
        "              print(f\"Epoch {epoch+1} Batch {batch_count+1}\")\n",
        "              history = model.train_on_batch(x, y, return_dict=True)\n",
        "              print(f'Loss: {history[\"loss\"]}')\n",
        "              batch_count += 1\n",
        "              if batch_count > batch_size:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UQz1WZF3atX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad764e2-5f75-459d-c250-b4842f4fa2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 1\n",
            "Loss: 9.900870323181152\n",
            "Epoch 1 Batch 2\n",
            "Loss: 9.882753372192383\n",
            "Epoch 1 Batch 3\n",
            "Loss: 9.875391006469727\n",
            "Epoch 1 Batch 4\n",
            "Loss: 9.859234809875488\n",
            "Epoch 1 Batch 5\n",
            "Loss: 9.850264549255371\n",
            "Epoch 1 Batch 6\n",
            "Loss: 9.805156707763672\n",
            "Epoch 1 Batch 7\n",
            "Loss: 9.815977096557617\n",
            "Epoch 1 Batch 8\n",
            "Loss: 9.828136444091797\n",
            "Epoch 1 Batch 9\n",
            "Loss: 9.787300109863281\n",
            "Epoch 1 Batch 10\n",
            "Loss: 9.779848098754883\n",
            "Epoch 1 Batch 11\n",
            "Loss: 9.730436325073242\n",
            "Epoch 1 Batch 12\n",
            "Loss: 9.762035369873047\n",
            "Epoch 1 Batch 13\n",
            "Loss: 9.665240287780762\n",
            "Epoch 1 Batch 14\n",
            "Loss: 9.739299774169922\n",
            "Epoch 1 Batch 15\n",
            "Loss: 9.745723724365234\n",
            "Epoch 1 Batch 16\n",
            "Loss: 9.66541862487793\n",
            "Epoch 1 Batch 17\n",
            "Loss: 9.54014778137207\n",
            "Epoch 1 Batch 18\n",
            "Loss: 9.502429962158203\n",
            "Epoch 1 Batch 19\n",
            "Loss: 9.53338623046875\n",
            "Epoch 1 Batch 20\n",
            "Loss: 9.642533302307129\n",
            "Epoch 1 Batch 21\n",
            "Loss: 9.65206527709961\n",
            "Epoch 1 Batch 22\n",
            "Loss: 9.521858215332031\n",
            "Epoch 1 Batch 23\n",
            "Loss: 9.465085983276367\n",
            "Epoch 1 Batch 24\n",
            "Loss: 9.446490287780762\n",
            "Epoch 1 Batch 25\n",
            "Loss: 9.423318862915039\n",
            "Epoch 1 Batch 26\n",
            "Loss: 9.450098037719727\n",
            "Epoch 1 Batch 27\n",
            "Loss: 9.37455940246582\n",
            "Epoch 1 Batch 28\n",
            "Loss: 9.358670234680176\n",
            "Epoch 1 Batch 29\n",
            "Loss: 9.257118225097656\n",
            "Epoch 1 Batch 30\n",
            "Loss: 9.41010856628418\n",
            "Epoch 1 Batch 31\n",
            "Loss: 9.196548461914062\n",
            "Epoch 1 Batch 32\n",
            "Loss: 9.22407341003418\n",
            "Epoch 1 Batch 33\n",
            "Loss: 9.273569107055664\n",
            "Epoch 1 Batch 34\n",
            "Loss: 9.300910949707031\n",
            "Epoch 1 Batch 35\n",
            "Loss: 9.381001472473145\n",
            "Epoch 1 Batch 36\n",
            "Loss: 9.347254753112793\n",
            "Epoch 1 Batch 37\n",
            "Loss: 9.166319847106934\n",
            "Epoch 1 Batch 38\n",
            "Loss: 9.331009864807129\n",
            "Epoch 1 Batch 39\n",
            "Loss: 9.025375366210938\n",
            "Epoch 1 Batch 40\n",
            "Loss: 9.187361717224121\n",
            "Epoch 1 Batch 41\n",
            "Loss: 9.160640716552734\n",
            "Epoch 1 Batch 42\n",
            "Loss: 9.007711410522461\n",
            "Epoch 1 Batch 43\n",
            "Loss: 9.225503921508789\n",
            "Epoch 1 Batch 44\n",
            "Loss: 9.074007034301758\n",
            "Epoch 1 Batch 45\n",
            "Loss: 8.982685089111328\n",
            "Epoch 1 Batch 46\n",
            "Loss: 9.057579040527344\n",
            "Epoch 1 Batch 47\n",
            "Loss: 8.971325874328613\n",
            "Epoch 1 Batch 48\n",
            "Loss: 9.098865509033203\n",
            "Epoch 1 Batch 49\n",
            "Loss: 9.011394500732422\n",
            "Epoch 1 Batch 50\n",
            "Loss: 9.066131591796875\n",
            "Epoch 1 Batch 51\n",
            "Loss: 8.991949081420898\n",
            "Epoch 1 Batch 52\n",
            "Loss: 8.776313781738281\n",
            "Epoch 1 Batch 53\n",
            "Loss: 8.711309432983398\n",
            "Epoch 1 Batch 54\n",
            "Loss: 8.852388381958008\n",
            "Epoch 1 Batch 55\n",
            "Loss: 8.935763359069824\n",
            "Epoch 1 Batch 56\n",
            "Loss: 8.496856689453125\n",
            "Epoch 1 Batch 57\n",
            "Loss: 9.157491683959961\n",
            "Epoch 1 Batch 58\n",
            "Loss: 8.976694107055664\n",
            "Epoch 1 Batch 59\n",
            "Loss: 9.008821487426758\n",
            "Epoch 1 Batch 60\n",
            "Loss: 9.123356819152832\n",
            "Epoch 1 Batch 61\n",
            "Loss: 8.878459930419922\n",
            "Epoch 1 Batch 62\n",
            "Loss: 8.845149993896484\n",
            "Epoch 1 Batch 63\n",
            "Loss: 8.787788391113281\n",
            "Epoch 1 Batch 64\n",
            "Loss: 8.803003311157227\n",
            "Epoch 1 Batch 65\n",
            "Loss: 8.698974609375\n",
            "Epoch 2 Batch 1\n",
            "Loss: 8.018624305725098\n",
            "Epoch 2 Batch 2\n",
            "Loss: 8.006149291992188\n",
            "Epoch 2 Batch 3\n",
            "Loss: 7.915553092956543\n",
            "Epoch 2 Batch 4\n",
            "Loss: 7.860836029052734\n",
            "Epoch 2 Batch 5\n",
            "Loss: 8.04699420928955\n",
            "Epoch 2 Batch 6\n",
            "Loss: 7.893788814544678\n",
            "Epoch 2 Batch 7\n",
            "Loss: 8.076234817504883\n",
            "Epoch 2 Batch 8\n",
            "Loss: 8.206746101379395\n",
            "Epoch 2 Batch 9\n",
            "Loss: 7.934696197509766\n",
            "Epoch 2 Batch 10\n",
            "Loss: 8.077427864074707\n",
            "Epoch 2 Batch 11\n",
            "Loss: 8.03759479522705\n",
            "Epoch 2 Batch 12\n",
            "Loss: 8.363340377807617\n",
            "Epoch 2 Batch 13\n",
            "Loss: 7.812947750091553\n",
            "Epoch 2 Batch 14\n",
            "Loss: 8.292951583862305\n",
            "Epoch 2 Batch 15\n",
            "Loss: 8.255592346191406\n",
            "Epoch 2 Batch 16\n",
            "Loss: 7.923656940460205\n",
            "Epoch 2 Batch 17\n",
            "Loss: 7.640148162841797\n",
            "Epoch 2 Batch 18\n",
            "Loss: 7.570018768310547\n",
            "Epoch 2 Batch 19\n",
            "Loss: 7.630784034729004\n",
            "Epoch 2 Batch 20\n",
            "Loss: 7.963916301727295\n",
            "Epoch 2 Batch 21\n",
            "Loss: 8.089030265808105\n",
            "Epoch 2 Batch 22\n",
            "Loss: 7.879914283752441\n",
            "Epoch 2 Batch 23\n",
            "Loss: 7.738652229309082\n",
            "Epoch 2 Batch 24\n",
            "Loss: 7.631594657897949\n",
            "Epoch 2 Batch 25\n",
            "Loss: 7.670772552490234\n",
            "Epoch 2 Batch 26\n",
            "Loss: 7.83799934387207\n",
            "Epoch 2 Batch 27\n",
            "Loss: 7.6316938400268555\n",
            "Epoch 2 Batch 28\n",
            "Loss: 7.681714057922363\n",
            "Epoch 2 Batch 29\n",
            "Loss: 7.451888084411621\n",
            "Epoch 2 Batch 30\n",
            "Loss: 7.777811050415039\n",
            "Epoch 2 Batch 31\n",
            "Loss: 7.3646955490112305\n",
            "Epoch 2 Batch 32\n",
            "Loss: 7.478609561920166\n",
            "Epoch 2 Batch 33\n",
            "Loss: 7.630472183227539\n",
            "Epoch 2 Batch 34\n",
            "Loss: 7.712558746337891\n",
            "Epoch 2 Batch 35\n",
            "Loss: 7.934013366699219\n",
            "Epoch 2 Batch 36\n",
            "Loss: 7.8831610679626465\n",
            "Epoch 2 Batch 37\n",
            "Loss: 7.582353115081787\n",
            "Epoch 2 Batch 38\n",
            "Loss: 7.870872497558594\n",
            "Epoch 2 Batch 39\n",
            "Loss: 7.388806343078613\n",
            "Epoch 2 Batch 40\n",
            "Loss: 7.649070739746094\n",
            "Epoch 2 Batch 41\n",
            "Loss: 7.670355796813965\n",
            "Epoch 2 Batch 42\n",
            "Loss: 7.444298267364502\n",
            "Epoch 2 Batch 43\n",
            "Loss: 7.789639472961426\n",
            "Epoch 2 Batch 44\n",
            "Loss: 7.532219886779785\n",
            "Epoch 2 Batch 45\n",
            "Loss: 7.4226274490356445\n",
            "Epoch 2 Batch 46\n",
            "Loss: 7.551348686218262\n",
            "Epoch 2 Batch 47\n",
            "Loss: 7.493941307067871\n",
            "Epoch 2 Batch 48\n",
            "Loss: 7.726927757263184\n",
            "Epoch 2 Batch 49\n",
            "Loss: 7.5566606521606445\n",
            "Epoch 2 Batch 50\n",
            "Loss: 7.706990718841553\n",
            "Epoch 2 Batch 51\n",
            "Loss: 7.599315643310547\n",
            "Epoch 2 Batch 52\n",
            "Loss: 7.238537788391113\n",
            "Epoch 2 Batch 53\n",
            "Loss: 7.244507789611816\n",
            "Epoch 2 Batch 54\n",
            "Loss: 7.437322616577148\n",
            "Epoch 2 Batch 55\n",
            "Loss: 7.534923553466797\n",
            "Epoch 2 Batch 56\n",
            "Loss: 6.997636795043945\n",
            "Epoch 2 Batch 57\n",
            "Loss: 7.951110363006592\n",
            "Epoch 2 Batch 58\n",
            "Loss: 7.663267135620117\n",
            "Epoch 2 Batch 59\n",
            "Loss: 7.712826251983643\n",
            "Epoch 2 Batch 60\n",
            "Loss: 7.901792526245117\n",
            "Epoch 2 Batch 61\n",
            "Loss: 7.581272602081299\n",
            "Epoch 2 Batch 62\n",
            "Loss: 7.575467109680176\n",
            "Epoch 2 Batch 63\n",
            "Loss: 7.542852401733398\n",
            "Epoch 2 Batch 64\n",
            "Loss: 7.587202072143555\n",
            "Epoch 2 Batch 65\n",
            "Loss: 7.406185626983643\n"
          ]
        }
      ],
      "source": [
        "fit_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Kr_mqRRB3SRN"
      },
      "outputs": [],
      "source": [
        "with open('./vectors.txt' ,'w') as f:\n",
        "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "    vectors = model.get_weights()[0]\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
        "        f.write('{} {}\\n'.format(word, str_vec))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_vM_y0Fc3SRO"
      },
      "outputs": [],
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC5fH0WE3SRO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hNK0Gaq33SRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2296d12-4913-4f8f-ebe7-70c6c1cbd677"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sandwich', 0.7488481402397156),\n",
              " ('sandwiches', 0.7221847772598267),\n",
              " ('bread', 0.700667679309845),\n",
              " ('pizzas', 0.7004502415657043),\n",
              " ('burger', 0.6977595686912537),\n",
              " ('snack', 0.6840099692344666),\n",
              " ('taco', 0.6788282990455627),\n",
              " ('pie', 0.6776915788650513),\n",
              " ('chicken', 0.6714670658111572),\n",
              " ('burgers', 0.66903156042099)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "w2v.most_similar(positive=['pizza'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VGn4aNYH3SRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342f4a43-c4c7-45b3-d191-09146d3ccd59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('grapes', 0.7915149927139282),\n",
              " ('wine', 0.7132691144943237),\n",
              " ('varieties', 0.7115378975868225),\n",
              " ('chardonnay', 0.7091464996337891),\n",
              " ('wines', 0.7076444029808044),\n",
              " ('pinot', 0.7055837512016296),\n",
              " ('zinfandel', 0.7001710534095764),\n",
              " ('vines', 0.681800127029419),\n",
              " ('sauvignon', 0.6581584811210632),\n",
              " ('cabernet', 0.6564268469810486)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "w2v.most_similar(positive=['grape'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkz7L2Lo3SRP"
      },
      "source": [
        "Do you notice the difference in the accuracy? For any task first search if there are any pretrained models to use!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMuG5oPdau7j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}